legend(0.2, 0.6, legend = c("Log", "LDA", "Lasso", "Ridge", "KNN"), col = 1:5, cex = 0.6, lty = 1)
}
plotlogrocinter
plotlogrocinter()
KNN.conf <- confusionMatrix(predVal.knn.inter, as.factor(data.agg.inter[!train,]$Fraud))
KNN.conf
plot(smotefit.tree, xlim = c(0, 0.05), ylim = c(0.84, 0.87))
plot(smotefit.tree, xlim = c(0, 0.05), ylim = c(0.84, 0.87), main = "Choosing Cp through CV")
plot(smotefit.tree, xlim = c(0, 0.05), ylim = c(0.84, 0.87), main = "Choosing Tree Cp through CV")
plot(roc.forest.smote, main = "RF ROC: SMOTE vs Original")
lines(roc.forest, col = "red")
legend(0.4, 0.4, legend = c("SMOTE", "Original"), col = c("black", "red"), lty = 1)
mtry <- sqrt(ncol(data.agg.np))
tunegrid <- expand.grid(.mtry=mtry)
smotefit.forest <- train(Fraud~.,
data=data.agg.np[train,],
method='rf',
metric='Accuracy',
tuneGrid=tunegrid,
trControl=trCtrl)
pred.forest <- predict(smotefit.forest, data.agg.np[!train, ], type = "prob")
predVal.ranForest <- predict(smotefit.forest, newdata = data.agg.np[!train,])
RF.conf <- confusionMatrix(predVal.ranForest, as.factor(data.agg.np[!train,]$Fraud))
roc.forest <-
plot(roc(data.agg.np[!train, FraudIndx], pred.forest[,2])
)
trCtrl <- trainControl(method='repeatedcv',
number=10,
repeats=3)
#Metric compare model is Accuracy
metric <- "Accuracy"
#Number randomely variable selected is mtry
mtry <- sqrt(ncol(data.agg.np))
tunegrid <- expand.grid(.mtry=mtry)
smotefit.forest <- train(Fraud~.,
data=smote_train_tree,
method='rf',
metric='Accuracy',
tuneGrid=tunegrid,
trControl=trCtrl)
pred.forest <- predict(smotefit.forest, data.agg.np[!train, ], type = "prob")
predVal.ranForest <- predict(smotefit.forest, newdata = data.agg.np[!train,])
RF.conf <- confusionMatrix(predVal.ranForest, as.factor(data.agg.np[!train,]$Fraud))
roc.forest.inter <- plot(roc(data.agg.np[!train, FraudIndx], pred.forest[,2]), main = "ROC: Tree model and extensions")
lines(roc(response = data.agg$Fraud[!train], predictor = pred.boosttree[,2]), col = "Blue") # think i need to pick my lamda better
lines(roc(data.agg.np[!train, FraudIndx], pred.tree[,2]), col = "Green")
legend(0.6, 0.4, legend = c("Tree", "Boosted Tree", "Random Forest"), col = c("Green", "Blue", "black"), lty = 1, lwd = 2)
plot(roc.forest.inter, main = "RF ROC: SMOTE vs Original")
lines(roc.forest, col = "red")
legend(0.4, 0.4, legend = c("SMOTE", "Original"), col = c("black", "red"), lty = 1)
lda.rocplot.inter
plotrocplotbest <- function() {
plot(log.rocplot.inter, col = 1, main = "ROC: Best plots")
lines(roc.forest.inter, col = 2)
lines(KNN.rocplot.inter, col = 3)
lines(lda.rocplot.inter, col = 4)
lines(svm.rocplot.inter, col = 5)
legend(0.2, 0.6, legend = c("Log", "RF", "KNN", "LDA", "SVM"), col = 1:5, cex = 0.6, lty = 1)
}
plotrocplotbest()
svm.rocplot.inter
log.rocplot.inter
roc.forest.inter
KNN.rocplot.inter
lda.rocplot.inter
svm.rocplot.inter
log.conf
RF.conf
KNN.conf
lda.conf
svm.conf
svm.rocplot.inter <- roc.svm.lin.inter # linear kernel is better
svm.rocplot.inter
svm.lin.conf
svm.rad.conf
svm.lin.conf
svm.rocplot.inter
plotrocplotbest <- function() {
plot(log.rocplot.inter, col = 1, main = "ROC: Best plots")
lines(roc.forest.inter, col = 2)
lines(KNN.rocplot.inter, col = 3)
lines(lda.rocplot.inter, col = 4)
lines(svm.rocplot.inter, col = 5)
legend(0.2, 0.6, legend = c("Log", "RF", "KNN", "LDA", "SVM"), col = 1:5, cex = 0.6, lty = 1)
}
plotrocplotbest()
plotrocplotbest <- function() {
plot(log.rocplot.inter, col = 1, main = "ROC: Best plots")
lines(roc.forest.inter, col = 2)
lines(KNN.rocplot.inter, col = 3)
lines(lda.rocplot.inter, col = 4)
lines(svm.rocplot.inter, col = 5)
legend(0.2, 0.6, legend = c("Log", "RF: Linear", "KNN", "LDA", "SVM"), col = 1:5, cex = 0.6, lty = 1)
}
plotrocplotbest()
plotrocplotbest <- function() {
plot(log.rocplot.inter, col = 1, main = "ROC: Best plots")
lines(roc.forest.inter, col = 2)
lines(KNN.rocplot.inter, col = 3)
lines(lda.rocplot.inter, col = 4)
lines(svm.rocplot.inter, col = 5)
legend(0.4, 0.6, legend = c("Log", "RF: Linear", "KNN", "LDA", "SVM"), col = 1:5, cex = 0.6, lty = 1)
}
plotrocplotbest()
boost.conf <- confusionMatrix(predVal.boosttree, as.factor(data.agg.np[!train,]$Fraud))
boost.conf
boosttree.rocplot.inter$auc
tree.conf <- confusionMatrix(predVal.tree, as.factor(data.agg.np[!train,]$Fraud))
tree.conf
tree.rocplot.inter
roc.forest.inter
lda.rocplot.inter
qda.rocplot.inter
ldapca.rocplot.inter
importance(smotefit.forest)
importance(smotefit.forest$finalModel)
plot(importance(smotefit.forest$finalModel))
barplot(importance(smotefit.forest$finalModel))
plot(importance(smotefit.forest$finalModel))
smotefit.forest$finalModel$importance
smotefit.forest$finalModel$votes
smotefit.forest$finalModel$param
smotefit.forest$finalModel$obsLevels
smotefit.forest$finalModel$inbag
smotefit.forest$finalModel$test
smotefit.forest$finalModel$ntree
smotefit.forest$finalModel$mtry
mtry
smotefit.forest$finalModel$predicted
smotefit.forest$finalModel$type
smotefit.forest$finalModel$call
smotefit.forest$finalModel$confusion
smotefit.forest$finalModel$votes
smotefit.forest$finalModel$oob.times
smotefit.forest$finalModel$classes
smotefit.forest$finalModel$proximity
smotefit.forest$finalModel$localImportance
smotefit.forest$finalModel$importance
plot(smotefit.forest$finalModel$importance)
barplot(smotefit.forest$finalModel$importance, las = 1)
barplot(smotefit.forest$finalModel$importance[,2], las = 1)
barplot(smotefit.forest$finalModel$importance[,1], las = 1)
barplot(smotefit.forest$finalModel$importance[,1], las = 3)
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest")
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.axis = 0.6)
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.6)
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.5)
par(mar = c(6, 4, 4, 1))
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.5)
par(mar = c(8, 4, 4, 1))
plot(roc.forest.inter, main = "RF ROC: SMOTE vs Original")
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.5)
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.8)
par(mar = c(9, 4, 4, 1))
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.8)
par(mar = c(10, 4, 4, 1))
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.8)
barplot(smotefit.forest$finalModel$importance[,1], las = 3, main = "Variable Importance in Random Forest", cex.names = 0.8, ylab = "Mean Decrease Gini")
FraudyCounty
library(randomForestExplainer)
install.packages(randomForestExplainer)
install.packages("randomForestExplainer")
library(randomForestExplainer) # for random forest graphs
explain_forest(smotefit.forest, interactions = TRUE, data = smote_train)
explain_forest(smotefit.forest$finalModel, interactions = TRUE, data = smote_train)
smotefit.forest <- train(Fraud~.,
data=smote_train_tree,
method='rf',
metric='Accuracy',
tuneGrid=tunegrid,
trControl=trCtrl,
local_imp = TRUE)
explain_forest(smotefit.forest$finalModel, interactions = TRUE, data = smote_train)
smotefit.forest$finalModel$tuneValue
smotefit.forest$finalModel$proximity
smotefit.forest$finalModel$\
smotefit.forest$finalModel$importanceSD
smotefit.forest$finalModel$forest
RF.conf <- confusionMatrix(predVal.ranForest, as.factor(data.agg.np[!train,]$Fraud))
RF.conf
normForest.conf <- confusionMatrix(normForest.conf, data.agg.np[!train, FraudIndx])
predVal.forest <- predict(fit.forest, data.agg.np[!train, ])
normForest.conf <- confusionMatrix(predVal.forest, data.agg.np[!train, FraudIndx])
normForest.conf
summary(backwards)
cv.fit.ridge$lambda.min
lambda.grid <- exp(seq(-8,0,length=100)) # grid of all lambdas to be tested
data.agg.inter.modelmatrix <- model.matrix(Fraud ~ ., data.agg.inter)[,-1]
fit.ridge <- glmnet(x = data.agg.inter.modelmatrix[train,], y = as.matrix(data.agg.inter[train,FraudIndx]), family = "binomial",
alpha=0, lambda=lambda.grid)
cv.fit.ridge <- cv.glmnet(x = data.agg.inter.modelmatrix[train,-FraudIndx],y = data.agg.inter[train,FraudIndx],
alpha=0, family = "binomial", lambda=lambda.grid, type.measure = "class") # type.measure = class gives misclassification error
plot(fit.ridge, xvar = "dev", label = TRUE)
plot(cv.fit.ridge)
ridge.coef <- coef(cv.fit.ridge, s = "lambda.min")
# ridge error
# type = response gives probabilities
# type = class gives predicition of either 1 or 0
ridge.predict <- predict(fit.ridge, s=cv.fit.ridge$lambda.min, newx = data.agg.inter.modelmatrix[!train,], type = "response")
ridge.predict <- as.numeric(ridge.predict)
ridge.rocplot.inter <- plot(roc(response = data.agg$Fraud[!train], predictor = ridge.predict), main = "Ridge ROC")
cv.fit.ridge$lambda.min
cv.fit.lasso$lambda.min
# Lasso regression (UNSURE IF THESE SHOULD BE IN THE BINOMIAL FAMILY, prediction outputs weird values)
lambda.grid <- exp(seq(-8,-2,length=100)) # grid of all lambdas to be tested
data.agg.inter.modelmatrix <- model.matrix(Fraud ~ ., data.agg.inter)[,-1]
fit.lasso <- glmnet(x = data.agg.inter.modelmatrix[train,], y = as.matrix(data.agg.inter[train,FraudIndx]), family = "binomial",
alpha=1, lambda=lambda.grid)
cv.fit.lasso <- cv.glmnet(x = data.agg.inter.modelmatrix[train,-FraudIndx],y = data.agg.inter[train,FraudIndx],alpha=1,
family = "binomial", lambda=lambda.grid, type.measure = "class") # type.measure = class gives misclassification error
plot(fit.lasso, xvar = "dev", label = TRUE, xlim = c(0.35, 0.42))
plot(cv.fit.lasso)
lasso.coef <- coef(cv.fit.lasso, s = "lambda.min")
# lasso error
# type = response gives probabilities
# type = class gives predicition of either 1 or 0
lasso.predict <- predict(fit.lasso, s=cv.fit.lasso$lambda.min, newx = data.agg.inter.modelmatrix[!train,], type = "response")
lasso.predict <- as.numeric(lasso.predict)
lasso.rocplot.inter <- plot(roc(response = data.agg$Fraud[!train], predictor = lasso.predict), main = "Penalised regression ROC")
lines(ridge.rocplot.inter, col = "red")
lines(log.rocplot.inter, col = "blue")
legend(0.4, 0.4, legend = c("Lasso", "Ridge", "Log Reg"), col = c("Black", "Red", "blue"), lty = 1)
lambda.grid <- exp(seq(-10,0,length=100)) # grid of all lambdas to be tested
data.agg.inter.modelmatrix <- model.matrix(Fraud ~ ., smote_train)[,-1]
fit.ridge <- glmnet(x = data.agg.inter.modelmatrix, y = as.matrix(smote_train[,FraudIndx]), family = "binomial",
alpha=0, lambda=lambda.grid)
cv.fit.ridge <- cv.glmnet(x = data.agg.inter.modelmatrix[,-FraudIndx],y = smote_train[,FraudIndx],
alpha=0, family = "binomial", lambda=lambda.grid, type.measure = "class") # type.measure = class gives misclassification error
plot(fit.ridge, xvar = "dev", label = TRUE)
plot(cv.fit.ridge)
ridge.coef <- coef(cv.fit.ridge, s = "lambda.min")
cv.fit.ridge$lambda.min
lambda.grid <- exp(seq(-8,-2,length=100)) # grid of all lambdas to be tested
data.agg.inter.modelmatrix <- model.matrix(Fraud ~ ., smote_train)[,-1]
fit.lasso <- glmnet(x = data.agg.inter.modelmatrix, y = as.matrix(smote_train[,FraudIndx]), family = "binomial",
alpha=1, lambda=lambda.grid)
cv.fit.lasso <- cv.glmnet(x = data.agg.inter.modelmatrix[,-FraudIndx],y = smote_train[,FraudIndx],alpha=1,
family = "binomial", lambda=lambda.grid, type.measure = "class") # type.measure = class gives misclassification error
plot(fit.lasso, xvar = "dev", label = TRUE, xlim = c(0.4, 0.6))
plot(cv.fit.lasso)
lasso.coef <- coef(cv.fit.lasso, s = "lambda.min")
# lasso error
# type = response gives probabilities
# type = class gives predicition of either 1 or 0
lasso.predict <- predict(fit.lasso, s=cv.fit.lasso$lambda.min, newx = data.agg.inter.modelmatrix[!train,], type = "response")
lasso.predict <- as.numeric(lasso.predict)
lasso.rocplot.inter <- plot(roc(response = data.agg$Fraud[!train], predictor = lasso.predict), main = "Penalised regression ROC")
cv.fit.lasso$lambda.min
ridge.coef <- coef(cv.fit.ridge, s = "lambda.min")
ridge.coef
lasso.coef
lasso.conf
lasso.rocplot.inter
log.rocplot.inter
lda.conf
control <- trainControl(method="repeatedcv", number=3, repeats = 3, verbose = TRUE, search = "grid")
tuneLength <- 10
metric <- "Accuracy"
fitlda <- train(Fraud~., data=smote_train, method="lda", metric=metric,
preProc=c("center", "scale"), trControl=control, tuneLength = tuneLength)
pred.lda <- predict(fitlda, newdata = data.agg.inter[!train,], type = "prob")
predVal.lda <- predict(fitlda, newdata = data.agg.inter[!train,])
lda.conf <- confusionMatrix(predVal.lda, as.factor(data.agg.inter[!train,]$Fraud))
lda.rocplot.inter <- plot(roc(response = data.agg$Fraud[!train], predictor = pred.lda[,1]), main = "LDA ROC")
ldamod <- fitlda$finalModel
largediff <- (ldamod$means[1,] - ldamod$means[2,])
largediff[abs(largediff) > 0.5] # those with have the largest effect on the mean
plot(ldamod$means[1, abs(largediff) > 0.5], type = "p", ylim = c(min(largediff), max(largediff)), lwd = 5,
xlab = "Predictors", ylab = "Contribution to Fraud probability",
xaxt = "n",
main = "Fraudulent prediction changes")
points(ldamod$means[2, abs(largediff) > 0.5], col = "red", lwd = 5)
axis(1, at = 1:length(names(largediff[abs(largediff) > 0.5])), labels = names(largediff[abs(largediff) > 0.5]), cex.axis = 0.6)
legend(5, 0.1, legend = c("No", "Yes"), col = c("Black", "Red"), pch = 1)
# lda (with pca)
control <- trainControl(method="repeatedcv", number=3, repeats = 3, verbose = TRUE, search = "grid")
tuneLength <- 10
metric <- "Accuracy"
# remove zero variance column race
fitlda <- train(Fraud~., data=smote_train[,-41], method="lda", metric=metric,
preProc=c("pca"), trControl=control, tuneLength = tuneLength)
pred.lda <- predict(fitlda, newdata = data.agg.inter[!train,], type = "prob")
predVal.lda <- predict(fitlda, newdata = data.agg.inter[!train,])
ldapca.conf <- confusionMatrix(predVal.lda, as.factor(data.agg.inter[!train,]$Fraud))
ldapca.rocplot.inter <- plot(roc(response = data.agg$Fraud[!train], predictor = pred.lda[,1]), main = "LDA ROC")
ldamod <- fitlda$finalModel
lda.conf
ldapca.rocplot.inter
lda.rocplot.inter
KNN.rocplot.inter
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(c(6, 4, 4, 1))
par(mar = c(6, 4, 4, 1))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(8, 4, 4, 1))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(8, 4, 4, 8))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(8, 4, 4, 8))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(1, 4, 4, 1))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(2, 4, 4, 1))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
par(mar = c(2, 5, 5, 1))
plot(smotefit.tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(smotefit.tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
fitlda
fitlda$bestTune
ldamod
knnFit.inter <- train(Fraud ~ ., data = smote_train,
method = "knn",
trControl = ctrl,
preProcess = c("center","scale"),
)
knnFit.inter <- train(Fraud ~ ., data = smote_train,
method = "knn",
trControl = ctrl,
preProcess = c("center","scale"),
tuneLength = 20)
plot(smotefit.boosttree)
smotefit.boosttree$bestTune
plot(roc.svm.lin.inter)
plot(smotefit.svm.lin.inter)
plot(smotefit.svm.lin.inter, main = "CV accuracy for Cost parameter")
svm.rocplot.inter <- roc.svm.lin.inter # linear kernel is better
plot(smotefit.svm.lin.inter, main = "CV accuracy for Cost parameter")
roc.svm.lin.inter <- roc(response = data.agg$Fraud[!train], predictor = pred.svm.lin.inter[,1])
roc(response = data.agg$Fraud[!train], predictor = pred.svm.lin.inter[,1])
roc.forest.inter <- plot(roc(data.agg.np[!train, FraudIndx], pred.forest[,2]), main = "ROC: Tree model and extensions")
roc(data.agg.np[!train, FraudIndx], pred.forest[,2])
smotefit.forest
smotefit.forest$finalModel
provider_eval <- read.csv("Medicare_Provider_Eval_PartB.csv", stringsAsFactors = FALSE)
eval <- read.csv("Medicare_Outpatient_Inpatient_Beneficiary_Eval_PartB.csv", stringsAsFactors = FALSE)
eval <- left_join(eval, fraudStatus)
# Cleaning eval
# note: these columns are all NA - remove them all
length(eval$ClmProcedureCode_6)
sum(is.na(eval$ClmProcedureCode_6))
length(eval$ClmProcedureCode_5)
sum(is.na(eval$ClmProcedureCode_5))
eval <- eval[,colSums(is.na(eval))<nrow(eval)] # removes full na columns
# changing those with dates to date type
date_vars = c(
"ClaimStartDt",
"ClaimEndDt",
"AdmissionDt",
"DischargeDt",
"DOB",
"DOD"
)
for (var in date_vars) {
eval[,var] =  as.Date(eval[,var])
}
# changing ClmProcedureCodes, race, state and county to character variables
# this avoids averaging them in later analysis
char_vars = c(
as.character(vars_select(colnames(eval), starts_with('Clm'))),
"Race",
"State",
"County"
)
for (var in char_vars) {
eval[,var] = as.character(eval[,var])
}
# Changing fraud from "yes"/"no" to 1/0
# can easily count
# Checking no NA's in Fraud column
eval$RenalDiseaseIndicator <- ifelse(eval$RenalDiseaseIndicator=="Y", 1, 0)
# Changing claimType from "yes"/"no" to 1/0
# can easily count
# Checking no NA's in claimType column
if(sum(is.na(eval$ClaimType)) == 0) {
print("Claimtype entries complete")
}
eval$ClaimType <- ifelse(eval$ClaimType=="Inpatient", 1, 0)
# rename column to inpatient
names(eval)[names(eval) == "ClaimType"] <- "Inpatient"
# Creating new variables for modelling
# num chronic conditions
eval <- eval %>%
mutate("ChronicCond_MixedNum" = 22 - rowSums(.[38:48])) # Hardcoded values for Chronic diseases
eval$ChronicCond_MixedNum <- as.integer(eval$ChronicCond_MixedNum)
# if code changes
eval <- eval %>%
mutate("changedCode" = (ifelse(ClmAdmitDiagnosisCode == ClmDiagnosisCode_1, 0, 1)))
eval$changedCode <- as.integer(eval$changedCode)
# Adding in the claim length in days
eval <- eval %>%
mutate(ClaimLength = ClaimEndDt - ClaimStartDt)
eval$ClaimLength <- as.numeric(eval$ClaimLength)
# sanity check on variables
variables = colnames(eval)
for (var in variables) {
print(var)
print(summary(na.omit(eval[,var])))
}
# calculating age (in years) at start of claim
# ASSUMPTION MADE - since there are many providers with no claim start dt
# the minimum start date was chosen of all claims
# Its assumed that a year difference doesnt have a significant effect
eval$Age <- as.numeric((eval$ClaimStartDt - eval$DOB)/365.25)
# Creates categorical variables for all variables.
# If they contain a NA value, it is set to the baseline value of a regression (e.g. usually 0)
# and the NA categorical value is set to 1
eval <- eval %>% mutate(hos_stay_len =  as.numeric(DischargeDt - AdmissionDt)) %>%
mutate(AttendingPhysicianNA = is.na(AttendingPhysician)) %>%
mutate(OperatingPhysicianNA = is.na(OperatingPhysician)) %>%
mutate(OperatingPhysicianNA = is.na(OperatingPhysician)) %>%
mutate(ClmDiagnosisCodeNA = is.na(ClmDiagnosisCode_1)) %>%
mutate(DODNA = !is.na(DOD)) %>%
mutate(changedCodeNA = is.na(changedCode) & Inpatient == 0) %>%
mutate(ClmAdmitDiagnosisCodeNA = is.na(ClmAdmitDiagnosisCode) &  Inpatient == 0)%>%
mutate(ClmProcedureCodeNA = is.na(ClmProcedureCode_1) & Inpatient == 1)
# INPATIENTS MAY NOT HAVE A START DATE - CHECK THE LENGTH
for(i in 1:length(eval)) {
if(any(is.na(eval[,i]))) { # if there is an na value in column
# indicatorVarName <- paste0(colnames(eval)[i], "NA")
#
# eval[[indicatorVarName]] <- as.integer(is.na(eval[,i]))
#
# replaces an na value in a numeric column with the minimum value
if(class(eval[, i]) != "character") { # checks to see if character first -> doesnt make sense to take a minimum of a char
eval[,i][is.na(eval[, i])] <- mean(na.omit(eval[,i]))
# better way to impute these. Create a linear regression model to approximate the value
# use caret preProcess
}
}
}
# Checks if physician is fraudulent
eval$FraudPhys <- match(as.character(eval$AttendingPhysician), FraudyPhys)
eval$FraudPhys[is.na(eval$FraudPhys)] <- 0
# Do the same as above for county and state
eval$FraudCounty <- match(as.character(eval$County), FraudyCounty)
eval$FraudCounty[is.na(eval$FraudCounty)] <- 0
eval$FraudState <- match(as.character(eval$State), FraudyState)
eval$FraudState[is.na(eval$FraudState)] <- 0
# Finds average claims per beneficiary for each provider
# introduce counties, race and state here
eval.bene <- eval %>%
group_by(BeneID, ProviderID, Race) %>%
summarise("NumClaims" = n())
eval.bene <- eval.bene %>%
group_by(ProviderID) %>%
summarise("AvgClaimsPerBene" = mean(NumClaims), "ModeRace" = mlv(Race, method = 'mfv', na.rm = TRUE)[1])
eval.bene$ModeRace <- as.factor(eval.bene$ModeRace)
# Aggregating data per provider
# Lose a lot of variables in this step (e.g. physicians etc). Need to think of a way to retain them
# Bootstrap thoughts
# Could potentially think of doing bootstrap to get std error of the averages
# would also give p-values on how reliable each average is
eval.agg <- aggregate(eval, by = list(eval$ProviderID), FUN = mean) # Takes the mean of all the other columns for each provider
eval.agg <- subset(eval.agg, select = -c(ProviderID))
names(eval.agg)[names(eval.agg) == "Group.1"] <- "ProviderID"
eval.agg$ProviderID <- as.character(eval.agg$ProviderID) # changing provider ID to character variable again
eval.agg2 <- aggregate(eval, by = list(eval$ProviderID), FUN = length) # Takes the number of claims per provider id
eval.agg <- eval.agg %>% mutate(num_clms = eval.agg2$BeneID) # mixes the two evalsets
eval.agg$num_clms <- as.double(eval.agg$num_clms) # Changes to double for later loop
eval.agg <- left_join(eval.agg, eval.bene) # adds average claims per benficiary
# Removes full NA columns
eval.agg <- eval.agg[,colSums(is.na(eval.agg))<nrow(eval.agg)]
# changes Fraud class to integer
eval.agg.np$Fraud <- as.integer(eval.agg.np$Fraud)
eval.agg.np <- eval.agg[,-1] # no providers
eval.agg.np <- eval.agg.np %>% # changes date to days since 2008-11-27 (GIVES SOME WEIRD VALUES)
mutate("StartDtdays_from_271108" = as.numeric(ClaimStartDt - as.Date("2008-11-27"))) %>%
mutate("EndDtdays_from_271108" = as.numeric(ClaimEndDt - as.Date("2008-11-27")))
eval.agg.np <- eval.agg.np[,-which(sapply(eval.agg.np, class) == "Date")]
# removes Claim start date, claim end date and some other variables deemed inappropriate for aggregated eval
# eval.agg <- eval.agg %>%
# dplyr::select(-c(ClaimStartDt, ClaimEndDt, AdmissionDt, DischargeDt, DOD, changedCode, ClaimLength, DOB))
#
# eval.agg.np <- eval.agg[,-1] # no providers
# Chosen model here is forest
eval_prob <- predict(smotefit.forest, eval.agg.np, type = "prob")[,2]
eval.agg$FraudProb <- eval_prob
eval.agg <- eval.agg[order(-eval.agg$FraudProb),]
top350 <- eval.agg[1:350,]
top350 <- top350[,1]
length(top350)
write.csv(top350,"C:/Users/Charlie/Documents/University/year3/T2/ACTL3142/Assignment/DataB", row.names = FALSE)
write.csv(top350,"C:/Users/Charlie/Documents/University/year3/T2/ACTL3142/Assignment/DataB", row.names = FALSE)
write.csv(top350,"C:\Users\Charlie\Documents\University\year3\T2\ACTL3142\Assignment\top350", row.names = FALSE)
write.csv(top350,"C:/Users/Charlie/Documents/University/year3/T2/ACTL3142/Assignment/top350", row.names = FALSE)
pwd
pwd()
getwd()
write.csv(top350, row.names = FALSE)
write.csv(top350,"C:/Users/Charlie/Documents/University/year3/T2/ACTL3142/Assignment/top350/top350.csv", row.names = FALSE)
top350 <- top350[,1]
top350
colnames(top350) <- "ProviderID"
length(unique(top350))
